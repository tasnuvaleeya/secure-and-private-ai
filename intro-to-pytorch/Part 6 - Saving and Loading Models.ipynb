{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms, datasets\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to normalize data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4fdd4f17b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc8AAAHPCAYAAAA1eFErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAL4ElEQVR4nO3dy26b5xWG0Y9HkdTBkuDCbgPDgTtIRvGg7Syj3GKBXE5GuQFn1qBOg44MwxIcS+ZBEsXeQdH3221opWvNNzZFk3z8j/Zgt9s1AOA/N9z3CwCAh0Y8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExr2D33z90jkWAB60775/NeiZ8+QJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASA03vcLAD5Njx8/7p5drVal3avlsnv2frcr7a4YDAal+d0Dfe37fN374skTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEHKSDH6jDg8PS/PjUf/Pw/nZWWn39MmT7tnXP/1U2l1RPc21z7NglfnRaFTa/fnz592z+/r39uQJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITc84RP1HBY+7/tUfGe5+3tbffscrUs7f7sD591zx4uFqXdH5e1115Rvcm5L797/Lg0f/ro9L/0Sn49njwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAISfJ4BO1mM9L8+Nx7et9UzhJdnR0VNo9Kbz2Z8+elXa/efOme3Y2m5V2V9631Xpd2n1+dtY9O51OS7tf/fBDaX4fPHkCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACH3POF/aDKZdM+ePHpU2r3b7Urzw2H//61PH52Wdi9Xy+7Z6h3TFy9edM9W75heXV11zw4GtWehzWbTPVt9z09OTrpnLy8vS7t7efIEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhJwkg3+jcpartdZOC2fFhoNBafdytSrNV89rVWy3992zk8n+TnO12hW49vbt2+7Zg4OD0u7KBbvN5qa0+3Cx6J51kgwAHgjxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITc8+Q3bzKZdM+enp6Wdo8K90DXlbuSrbVWvAc6m826Z29uaq99NqvdpqyYz+fds6VboK21yWTaPVt53a21tircf725rd3zPDo6Ls3vgydPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQMhJMn4Vw8JprqPDw9Lu0bj/Y357Uzu1tB2N+me329Luw8WiND876D9Jtr2vvfbRsPC+FXcPCqfclstlafd43P93L5f9J8Vaa22zWXfP7na70u6zs7Pu2crJwQpPngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJAyD3P/yMHBwf9s9NpafeocNeycl+xtdqtwWHhdbdW+7unk9p7PijcUG2ttXeXF92zi/m8tHs2678lenR4VNpduQe62WxKu8fj/tuUy+XH0u7ptP/3Yddq9zzX6/5bovPiZ62XJ08ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBILS3e56VG43D4p3CyaT/Zl51d+W+Y2W2tdp7vs+bmvf396XdlZuc1b+7Yo+rW2utTcb9Pw93d3el3RcX/bdEr66vS7tHhe94/fPS/z1ZLBalzZXfxcr3u2pf31FPngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBIDQ3k6SVc7fzGaz0u5B6z9hMz2YlnZXzmtti2eeKsaF81StVQ4ttbYrniSr3PaqnqCrvG/j4gm64bA2Px73z2+329Lu4+Pjve2unFOr/K5Vd1c/q/s7KlY7aeYkGQA8EOIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACHUfG/zyiy9Kiw8Xh92z7y4vSrsvLvrn379/X9pduVs3Kt7rmx4c9A/v6WZea60Ni7tHhbuYk+Id04rK7dfWWru9vS3Nr9b9n9Xq7dnZbN49u16vSrsHhe9Z9bZk5X2r7q7cIr25uSntPjs/755drWr/3r08eQJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC3feW/vbjj6XFf3zxonu2fg5t0T273mxKu6+urrpnf/nlQ2n3hw/986vimad9qpwkGxVPkk0LZ56Ojo5Ku4+Pj0vzle/JdDot7a6c1zo+PintXi4/ds+enNR2j/d4Aq9y2qt6Du33T592z/71229Lu3t58gSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQt3H43a7XWnx31+/3sts1Xw+L80/ffKke/b8/Ly0+/Pnz7tnq7cl7+5uu2fn8/67kq21dn9/3z273d6Vdm9ubrpnrwu3X1tr7d27d6X5fy6X3bPX19el3ReXl92zL7/6qrT7L3/6c/fs9cf+W6Cttbbb9d/UrHzOW2vt7q7/s17d/Y+ff+6eXa/Xpd29PHkCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQoPe02LffP2ydpMMAPbsu+9fDXrmPHkCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAiJJwCExBMAQuIJACHxBICQeAJASDwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC4gkAIfEEgJB4AkBIPAEgJJ4AEBJPAAgNdrvdvl8DADwonjwBICSeABASTwAIiScAhMQTAELiCQAh8QSAkHgCQEg8ASAkngAQEk8ACIknAITEEwBC/wJ94FJVd2ZOCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "helper.imshow(images[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.682..  Test Loss: 0.983..  Test Accuracy: 0.667\n",
      "Epoch: 1/2..  Training Loss: 1.028..  Test Loss: 0.739..  Test Accuracy: 0.729\n",
      "Epoch: 1/2..  Training Loss: 0.869..  Test Loss: 0.706..  Test Accuracy: 0.739\n",
      "Epoch: 1/2..  Training Loss: 0.775..  Test Loss: 0.628..  Test Accuracy: 0.752\n",
      "Epoch: 1/2..  Training Loss: 0.738..  Test Loss: 0.616..  Test Accuracy: 0.764\n",
      "Epoch: 1/2..  Training Loss: 0.669..  Test Loss: 0.588..  Test Accuracy: 0.771\n",
      "Epoch: 1/2..  Training Loss: 0.718..  Test Loss: 0.598..  Test Accuracy: 0.775\n",
      "Epoch: 1/2..  Training Loss: 0.676..  Test Loss: 0.616..  Test Accuracy: 0.755\n",
      "Epoch: 1/2..  Training Loss: 0.666..  Test Loss: 0.561..  Test Accuracy: 0.791\n",
      "Epoch: 1/2..  Training Loss: 0.633..  Test Loss: 0.556..  Test Accuracy: 0.798\n",
      "Epoch: 1/2..  Training Loss: 0.628..  Test Loss: 0.527..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.647..  Test Loss: 0.513..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.577..  Test Loss: 0.518..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.631..  Test Loss: 0.509..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.581..  Test Loss: 0.496..  Test Accuracy: 0.815\n",
      "Epoch: 1/2..  Training Loss: 0.591..  Test Loss: 0.498..  Test Accuracy: 0.816\n",
      "Epoch: 1/2..  Training Loss: 0.581..  Test Loss: 0.495..  Test Accuracy: 0.820\n",
      "Epoch: 1/2..  Training Loss: 0.584..  Test Loss: 0.506..  Test Accuracy: 0.814\n",
      "Epoch: 1/2..  Training Loss: 0.612..  Test Loss: 0.485..  Test Accuracy: 0.817\n",
      "Epoch: 1/2..  Training Loss: 0.600..  Test Loss: 0.474..  Test Accuracy: 0.827\n",
      "Epoch: 1/2..  Training Loss: 0.573..  Test Loss: 0.500..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.556..  Test Loss: 0.481..  Test Accuracy: 0.822\n",
      "Epoch: 1/2..  Training Loss: 0.540..  Test Loss: 0.478..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.576..  Test Loss: 0.474..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.553..  Test Loss: 0.485..  Test Accuracy: 0.815\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.470..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.568..  Test Loss: 0.488..  Test Accuracy: 0.822\n",
      "Epoch: 2/2..  Training Loss: 0.548..  Test Loss: 0.470..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.562..  Test Loss: 0.464..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.480..  Test Accuracy: 0.817\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.464..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.518..  Test Loss: 0.461..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.501..  Test Loss: 0.466..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.555..  Test Loss: 0.472..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.520..  Test Loss: 0.485..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.463..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.535..  Test Loss: 0.456..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.544..  Test Loss: 0.461..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.519..  Test Loss: 0.456..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.539..  Test Loss: 0.447..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.453..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.489..  Test Loss: 0.465..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.483..  Test Loss: 0.459..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.548..  Test Loss: 0.442..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.503..  Test Loss: 0.439..  Test Accuracy: 0.845\n",
      "Epoch: 2/2..  Training Loss: 0.494..  Test Loss: 0.471..  Test Accuracy: 0.834\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Model \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Our Model \\n\\n\", model, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict using ```torch.load```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict into the Network, ```model.load_state_dict(state_dict)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5530b1a255ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/udacity/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 777\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'input_size': 784,\n",
    "    'output_size': 10,\n",
    "    'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "    'state_dict': model.state_dict()\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                            checkpoint['output_size'],\n",
    "                            checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(udacity-local)",
   "language": "python",
   "name": "udacity-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
